{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_metrics(data_path, num_buckets=10):\n",
    "    df = pd.read_csv(data_path)\n",
    "    gold = df['exact_matches'].values.tolist()\n",
    "    conf = df['confidences'].values.tolist()\n",
    "    thresholds = np.quantile(conf, np.arange(0,1,1.0/num_buckets)).tolist()\n",
    "    prediction_metrics = {}\n",
    "    for metric in [\"tp\",\"tn\",\"fp\",\"fn\"]:\n",
    "        prediction_metrics[metric] = np.zeros_like(thresholds)\n",
    "    for bucket, threshold in enumerate(thresholds):\n",
    "        for i, conf_score in enumerate(conf):\n",
    "            em = gold[i]\n",
    "            if conf_score >= threshold:\n",
    "                if em >= 0.5:\n",
    "                    prediction_metrics['tp'][bucket] += 1\n",
    "                else:\n",
    "                    prediction_metrics['fp'][bucket] += 1\n",
    "            else:\n",
    "                if em < 0.5:\n",
    "                    prediction_metrics['tn'][bucket] += 1\n",
    "                else:\n",
    "                    prediction_metrics['fn'][bucket] += 1\n",
    "    prediction_metrics['included'] = (prediction_metrics['tp'] + prediction_metrics['fp']) / (\n",
    "        prediction_metrics['tp'] + prediction_metrics['fp'] + prediction_metrics['tn'] + prediction_metrics['fn'])\n",
    "    prediction_metrics['prediction_acc'] = (prediction_metrics['tp'] + prediction_metrics['tn']) / (\n",
    "        prediction_metrics['tp'] + prediction_metrics['fp'] + prediction_metrics['tn'] + prediction_metrics['fn'])\n",
    "    prediction_metrics['precision'] = prediction_metrics['tp'] / (prediction_metrics['tp'] + prediction_metrics['fp'])\n",
    "    prediction_metrics['recall'] = prediction_metrics['tp'] / (prediction_metrics['tp'] + prediction_metrics['fn'])\n",
    "    prediction_metrics['F1'] = 2 * prediction_metrics['tp'] / (2 * prediction_metrics['tp'] + prediction_metrics['fp'] +  + prediction_metrics['fn'])\n",
    "    \n",
    "    for i in range(num_buckets):\n",
    "        if 1 - prediction_metrics['included'][i] < float(i) / num_buckets:\n",
    "            for metric in prediction_metrics:\n",
    "                prediction_metrics[metric][i] = 0\n",
    "    \n",
    "    return prediction_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage-Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = get_prediction_metrics('../diagrams/baseline_raw.csv')\n",
    "dropout_metrics = get_prediction_metrics('../diagrams/dropout_2_beam_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=np.arange(0,1,0.1), y=np.flip(baseline_metrics['precision']), name='Baseline calibration'))\n",
    "fig.add_trace(go.Bar(x=np.arange(0,1,0.1), y=np.flip(dropout_metrics['precision']), name='Best calibration'))\n",
    "fig.update_layout(xaxis_title='Fraction of data included',\n",
    "                  yaxis_title='Precision')\n",
    "fig.update_yaxes(range=(0,1))\n",
    "fig.update_layout(xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickformat = ',.0%',\n",
    "        tickvals = np.arange(0,1,0.1)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = get_prediction_metrics('../diagrams/baseline_raw.csv', num_buckets=100)\n",
    "dropout_metrics = get_prediction_metrics('../diagrams/dropout_2_beam_raw.csv', num_buckets=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=baseline_metrics['recall'], y= baseline_metrics['precision'], name='Baseline', mode='markers'))\n",
    "fig.add_trace(go.Scatter(x=dropout_metrics['recall'], y= dropout_metrics['precision'], name='Best Calibrator', mode='markers'))\n",
    "fig.update_layout(title='Calibration Precision-Recall',\n",
    "                   xaxis_title='Recall',\n",
    "                   yaxis_title='Precision')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
